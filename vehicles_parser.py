import cloudscraper
from bs4 import BeautifulSoup
import json
import time
import re

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏
BASE_DOMAIN = "https://arz-wiki.com"
BASE_URL = "https://arz-wiki.com/arz-rp/vehicles/"

# –ú–µ–Ω—è–µ–º –º–∞—Å–∫–∏—Ä–æ–≤–∫—É –Ω–∞ Firefox (–∏–Ω–æ–≥–¥–∞ –ø–æ–º–æ–≥–∞–µ—Ç –æ—Ç Cloudflare –Ω–∞ —Å–µ—Ä–≤–µ—Ä–∞—Ö)
scraper = cloudscraper.create_scraper(
    browser={
        'browser': 'firefox',
        'platform': 'windows',
        'desktop': True
    }
)

def clean_text(text):
    if not text: return ""
    return re.sub(r'\s+', ' ', text).strip()

def fix_vehicle_name(raw_name):
    if not raw_name: return "Unknown"
    
    # 1. –£–±–∏—Ä–∞–µ–º SEO-–º—É—Å–æ—Ä ("üöó –¶–µ–Ω—ã –∏ —Å–∫–æ—Ä–æ—Å—Ç—å", "2026", "–Ω–∞ Arizona RP")
    name = re.sub(r'üöó|–¶–µ–Ω—ã –∏ —Å–∫–æ—Ä–æ—Å—Ç—å|202\d|–Ω–∞ Arizona RP|‚Äî ARZ-WIKI', '', raw_name)
    
    # 2. –£–±–∏—Ä–∞–µ–º ID –≤ —Å–∫–æ–±–∫–∞—Ö, –µ—Å–ª–∏ –æ–Ω –µ—Å—Ç—å –≤ –Ω–∞–∑–≤–∞–Ω–∏–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä "–ß—É–±–í–æ–∑ (15765)")
    name = re.sub(r'\(\d+\)', '', name)
    
    # 3. –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã
    return clean_text(name)

def parse_vehicle_page(url):
    try:
        response = scraper.get(url)
        
        # –ï—Å–ª–∏ –∑–∞—â–∏—Ç–∞ –≤–µ—Ä–Ω—É–ª–∞ 403 –∏–ª–∏ 503
        if response.status_code not in [200, 404]:
            print(f"[-] –ë–ª–æ–∫ Cloudflare ({response.status_code}): {url}")
            return None
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –∫–∞–ø—á—É
        page_title = soup.title.text if soup.title else ""
        if "Just a moment" in page_title or "Attention Required" in page_title:
            print(f"[-] –ü–æ–π–º–∞–ª–∏ –∫–∞–ø—á—É –Ω–∞ {url}. –ü—Ä–æ–ø—É—Å–∫.")
            return None

        # --- 1. –ù–ê–ó–í–ê–ù–ò–ï ---
        name = "Unknown"
        h1 = soup.find('h1', class_='entry-title')
        if h1:
            name = clean_text(h1.text)
        elif soup.title:
            name = clean_text(soup.title.text)
            
        name = fix_vehicle_name(name)

        if name in ["–¢—Ä–∞–Ω—Å–ø–æ—Ä—Ç", "Vehicles", "Unknown"]:
            return None

        # --- 2. –•–ê–†–ê–ö–¢–ï–†–ò–°–¢–ò–ö–ò ---
        specs = {}
        rows = soup.find_all('tr')
        has_data = False # –§–ª–∞–≥, –Ω–∞—à–ª–∏ –ª–∏ –º—ã —Ö–æ—Ç—å —á—Ç–æ-—Ç–æ
        
        for row in rows:
            cols = row.find_all(['td', 'th'])
            if len(cols) == 2:
                key = clean_text(cols[0].text).replace(':', '')
                val = clean_text(cols[1].text)
                specs[key] = val
                has_data = True

        # !!! –í–ê–ñ–ù–ê–Ø –ü–†–û–í–ï–†–ö–ê !!!
        # –ï—Å–ª–∏ —Ç–∞–±–ª–∏—Ü–∞ –ø—É—Å—Ç–∞—è (has_data == False), –∑–Ω–∞—á–∏—Ç —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –±–∏—Ç–∞—è –∏–ª–∏ –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω–∞.
        # –ú—ã –ù–ï —Å–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–∞–∫—É—é –º–∞—à–∏–Ω—É.
        if not has_data:
            print(f"[-] –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö (–ø—É—Å—Ç–∞—è —Ç–∞–±–ª–∏—Ü–∞): {name}")
            return None

        vehicle_data = {
            'name': name,
            'url': url,
            'speed': specs.get('C–∫–æ—Ä–æ—Å—Ç—å', '-'),
            'speed_tt': specs.get('C–∫–æ—Ä–æ—Å—Ç—å c TT2', '-'),
            'speed_ft': specs.get('C–∫–æ—Ä–æ—Å—Ç—å —Å –§–¢ (red)', '-'),
            'accel': specs.get('–†–∞–∑–≥–æ–Ω', '-'),
            'accel_100': specs.get('–†–∞–∑–≥–æ–Ω–∞ –¥–æ 100–∫–º', '-'),
            'seats': specs.get('–ú–µ—Å—Ç –≤ –º–∞—à–∏–Ω–µ', '-'),
            'type': specs.get('–¢–∏–ø', '-'),
            'model_id': specs.get('ID –º–∞—à–∏–Ω—ã', '-'),
            'game_name': specs.get('–ò–≥—Ä–æ–≤–æ–µ –∏–º—è', '-'),
            'files': specs.get('–§–∞–π–ª—ã', '-')
        }

        # --- 3. –û–ü–ò–°–ê–ù–ò–ï ---
        content_div = soup.find('div', class_='entry-content')
        description_lines = []
        if content_div:
            for elem in content_div.find_all(['p', 'li']):
                if not elem.find_parent('table'):
                    text = clean_text(elem.text)
                    if len(text) > 3 and "C–∫–æ—Ä–æ—Å—Ç—å" not in text:
                        description_lines.append(text)
        
        vehicle_data['description'] = "\n".join(description_lines)

        print(f"[+] –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {name}")
        return vehicle_data

    except Exception as e:
        print(f"[-] –û—à–∏–±–∫–∞: {e}")
        return None

def get_all_vehicles():
    all_vehicles = []
    page = 1
    
    print("=== –ü–ê–†–°–ò–ù–ì –ú–ê–®–ò–ù (VER 3.0) ===")

    while True:
        if page == 1: url = BASE_URL
        else: url = f"{BASE_URL}page/{page}/"
            
        print(f"\n>>> –°—Ç—Ä–∞–Ω–∏—Ü–∞ {page}...")
        
        try:
            response = scraper.get(url)
            if response.status_code != 200:
                print("–û—à–∏–±–∫–∞ –¥–æ—Å—Ç—É–ø–∞ –∏–ª–∏ –∫–æ–Ω–µ—Ü —Å—Ç—Ä–∞–Ω–∏—Ü.")
                break

            soup = BeautifulSoup(response.text, 'html.parser')
            links = soup.find_all('a', href=True)
            vehicle_links = []
            
            for link in links:
                href = link['href']
                if href.startswith("/"): href = BASE_DOMAIN + href
                
                if "/vehicles/" in href and href != BASE_URL and "/page/" not in href and "/category/" not in href:
                    if href not in vehicle_links:
                        vehicle_links.append(href)

            print(f"–°—Å—ã–ª–æ–∫ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ: {len(vehicle_links)}")
            
            if not vehicle_links:
                print("–ú–∞—à–∏–Ω –Ω–µ—Ç. –ó–∞–≤–µ—Ä—à–∞–µ–º.")
                break

            for v_url in vehicle_links:
                data = parse_vehicle_page(v_url)
                if data:
                    all_vehicles.append(data)
                time.sleep(0.5) # –ü–∞—É–∑–∞ –≤–∞–∂–Ω–∞

            page += 1
            
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ —Ü–∏–∫–ª–∞: {e}")
            break

    print(f"\n–ò–¢–û–ì: {len(all_vehicles)} –º–∞—à–∏–Ω.")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º
    with open('vehicles.json', 'w', encoding='utf-8') as f:
        json.dump(all_vehicles, f, ensure_ascii=False, indent=4)

if __name__ == "__main__":
    get_all_vehicles()
